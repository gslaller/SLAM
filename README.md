# SLAM: Simultaneous Localisation and Mapping

This project is a Python implementation of a neural network-based simultaneous localization and mapping (NN-SLAM) algorithm that takes a video as input and estimates the camera position and depth map at each frame. The algorithm uses convolutional neural networks (CNNs) with a U-Net architecture and the PyTorch framework to address the three main challenges of classical SLAM: camera pose estimation, depth map creation, and loop closure detection.

- **camera pose** estimation component of the algorithm leverages the latent vector generated by a CNN to determine the camera's position and orientation relative to the previous frame. This is achieved by identifying distinctive features in the camera's images and using them to perform triangulation through a process known as epipolar geometry.

- **depth map** generation component uses a U-Net architecture to provide a latent vector for each pixel in the image. This vector is then used in a gradient descent procedure to find matching points between different frames. The resulting depth map is generated by combining this information with the camera pose estimates and assuming that the scene is static. However, the algorithm does not currently account for moving objects.

- **Loop closure** detection component of the algorithm uses the latent vector of an image frame generated by the CNN to detect when the device has returned to a previously visited location and update the map accordingly. This helps to correct for drift and variations in the viewing angle.

## Requirements

This project requires Python 3.8 or higher and PyTorch +1.0 or higher. You also need to install the following Python packages:
- numpy
- matplotlib (optional, for plotting)
- opencv-python (video reading and writing -depth map-)

## Usage

To run the SLAM algorithm on a video file, use the following command:
```bash
python slam.py --video <video_file> [--output <output_file>]
```

